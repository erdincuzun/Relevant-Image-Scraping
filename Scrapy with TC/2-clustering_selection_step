import random
import Levenshtein.StringMatcher as lev 
import numpy as np
from sklearn.cluster import dbscan
import math

# find clusters with dbscan and select random pages from each cluster

def find_clusters(clus_size = 5):
    def clus_to_dict(clustering):
        d = {}
        i = 0
        for clus in clustering[1]:
            if clus in d:
                d[clus].append(i)
            else:
                d[clus] = [i]
            i += 1
        return d

    def lev_metric(x, y):
        i, j = int(x[0]), int(y[0])     # extract indices
        return lev.distance(data[i], data[j])
    
    data = []
    with open("URLs.csv", "r") as f:
        data = f.readlines()
    
    d_list = [item for item in range(0, len(data))]
    
    X = np.arange(len(data)).reshape(-1, 1)

    pos_eps, pos_minsamples, upSize = 0.5, 1, 0.5
    selected_clus_d = {}
    while True:
        clustering = dbscan(X, metric=lev_metric, eps=pos_eps, min_samples=pos_minsamples, algorithm='auto')
        d = clus_to_dict(clustering)

        # print(len(d), " : ", len(selected_clus_d), ", ", pos_eps, ", ", pos_minsamples)
        if clus_size == len(d):
            selected_clus_d = d.copy()
            break #clusters ok
        elif len(selected_clus_d) == 0:
            selected_clus_d = d.copy()
        elif abs(len(d) - clus_size) < abs(len(selected_clus_d) - clus_size): #closer cluster
            selected_clus_d = d.copy()
        else:
            upSize += 0.5
            if upSize > 3: #
                pos_eps = 30.5
        
        if pos_eps > 30:
            pos_minsamples += 1
            if pos_minsamples == 6: 
                break
            pos_eps = 0.5
            upSize = 0.5
        else:
            pos_eps += upSize
    
    selected_pages = []
    d_count = {}
    for key in selected_clus_d:
        d_count[key] = len(selected_clus_d[key])
        selected_pages.append(d_list[random.choice(selected_clus_d[key])])

    rest_clus = clus_size - len(selected_pages)
    lenData = len(data)
    if rest_clus > 0:
        for key in d_count.keys():
            d_count[key] = d_count[key] * rest_clus / lenData

        sort_orders = sorted(d_count.items(), key=lambda x: x[1], reverse=True)
        for i in sort_orders:
            if i[1] > 0:
                for j in range(math.ceil(i[1])):
                    selected_pages.append(d_list[random.choice(selected_clus_d[i[0]])])
            if clus_size == len(selected_pages):
                break

    return selected_pages, list(data[i].replace('\n', '') for i in selected_pages), clus_size

#output: clustersize: 5 or 6 (we recommend these values with our tests)
pages, urls, clus_size = find_clusters(5)
print(urls, clus_size)

#result: 
# 'https://www.dailysabah.com/world/europe/royal-ceremony-proclaims-charles-as-king-queens-funeral-on-sept-19', 
# 'https://www.dailysabah.com/search?qsection=world&pgno=95', 
# 'https://www.dailysabah.com/search?qsection=world&pgno=105', 
# 'https://www.dailysabah.com/world/new-covid-cases-deaths-on-decline-worldwide-who/news', 
# 'https://www.dailysabah.com/world/mid-east/iran-iaea-standoff-last-major-hurdle-in-reviving-nuke-deal'
# 5
